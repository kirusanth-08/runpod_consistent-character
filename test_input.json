{
  "input": {
    "workflow": {
      "3": {
        "inputs": {
          "seed": 606382063316326,
          "steps": 8,
          "cfg": 1,
          "sampler_name": "euler",
          "scheduler": "simple",
          "denoise": 1,
          "model": [
            "75",
            0
          ],
          "positive": [
            "104",
            0
          ],
          "negative": [
            "106",
            0
          ],
          "latent_image": [
            "88",
            0
          ]
        },
        "class_type": "KSampler",
        "_meta": {
          "title": "KSampler"
        }
      },
      "8": {
        "inputs": {
          "samples": [
            "3",
            0
          ],
          "vae": [
            "39",
            0
          ]
        },
        "class_type": "VAEDecode",
        "_meta": {
          "title": "VAE Decode"
        }
      },
      "38": {
        "inputs": {
          "clip_name": "qwen_2.5_vl_7b_fp8_scaled.safetensors",
          "type": "qwen_image",
          "device": "default"
        },
        "class_type": "CLIPLoader",
        "_meta": {
          "title": "Load CLIP"
        }
      },
      "39": {
        "inputs": {
          "vae_name": "qwen_image_vae.safetensors"
        },
        "class_type": "VAELoader",
        "_meta": {
          "title": "Load VAE"
        }
      },
      "60": {
        "inputs": {
          "filename_prefix": "ComfyUI",
          "images": [
            "8",
            0
          ]
        },
        "class_type": "SaveImage",
        "_meta": {
          "title": "Save Image"
        }
      },
      "66": {
        "inputs": {
          "shift": 3,
          "model": [
            "89",
            0
          ]
        },
        "class_type": "ModelSamplingAuraFlow",
        "_meta": {
          "title": "ModelSamplingAuraFlow"
        }
      },
      "75": {
        "inputs": {
          "strength": 1,
          "model": [
            "66",
            0
          ]
        },
        "class_type": "CFGNorm",
        "_meta": {
          "title": "CFGNorm"
        }
      },
      "88": {
        "inputs": {
          "pixels": [
            "108",
            0
          ],
          "vae": [
            "39",
            0
          ]
        },
        "class_type": "VAEEncode",
        "_meta": {
          "title": "VAE Encode"
        }
      },
      "89": {
        "inputs": {
          "lora_name": "Qwen-Image-Edit-Lightning-8steps-V1.0-bf16.safetensors",
          "strength_model": 1,
          "model": [
            "102",
            0
          ]
        },
        "class_type": "LoraLoaderModelOnly",
        "_meta": {
          "title": "LoraLoaderModelOnly"
        }
      },
      "93": {
        "inputs": {
          "upscale_method": "lanczos",
          "megapixels": 1,
          "image": [
            "103",
            0
          ]
        },
        "class_type": "ImageScaleToTotalPixels",
        "_meta": {
          "title": "Scale Image to Total Pixels"
        }
      },
      "102": {
        "inputs": {
          "unet_name": "Qwen-Image-Edit-2509-Q8_0.gguf"
        },
        "class_type": "UnetLoaderGGUF",
        "_meta": {
          "title": "Unet Loader (GGUF)"
        }
      },
      "103": {
        "inputs": {
          "image": "person.jpg"
        },
        "class_type": "LoadImage",
        "_meta": {
          "title": "Load Image"
        }
      },
      "104": {
        "inputs": {
          "prompt": "swap the award with this bottle from his hand.",
          "clip": [
            "38",
            0
          ],
          "vae": [
            "39",
            0
          ],
          "image1": [
            "93",
            0
          ],
          "image2": [
            "108",
            0
          ]
        },
        "class_type": "TextEncodeQwenImageEditPlus",
        "_meta": {
          "title": "TextEncodeQwenImageEditPlus"
        }
      },
      "106": {
        "inputs": {
          "prompt": "No extra fingers, non natural things, plastic skin",
          "clip": [
            "38",
            0
          ],
          "vae": [
            "39",
            0
          ],
          "image1": [
            "93",
            0
          ],
          "image2": [
            "108",
            0
          ]
        },
        "class_type": "TextEncodeQwenImageEditPlus",
        "_meta": {
          "title": "TextEncodeQwenImageEditPlus"
        }
      },
      "108": {
        "inputs": {
          "upscale_method": "lanczos",
          "megapixels": 1,
          "image": [
            "109",
            0
          ]
        },
        "class_type": "ImageScaleToTotalPixels",
        "_meta": {
          "title": "Scale Image to Total Pixels"
        }
      },
      "109": {
        "inputs": {
          "image": "item.jpg"
        },
        "class_type": "LoadImage",
        "_meta": {
          "title": "Load Image"
        }
      }
    },
    "images": [
      {
        "name": "person.jpg",
        "image": "<base64 encoded person image>"
      },
      {
        "name": "item.jpg",
        "image": "<base64 encoded item image>"
      }
    ]
  }
}
